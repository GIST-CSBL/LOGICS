{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175cf14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logics_pack import global_settings, chemistry, generator, analysis, smiles_vocab, smiles_lstm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "project_paths = global_settings.build_project_paths(project_dir='../')\n",
    "expset_obj = global_settings.ExperimentSettings(project_paths['EXPERIMENT_SETTINGS_JSON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7ce7e",
   "metadata": {},
   "source": [
    "We first build the pre-training dataset. \n",
    "\n",
    "From the ChEMBL dataset, we exclude the molecules from validation and test set of the predictor bioassay datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_aff = pd.read_csv(project_paths['KOR_DATA_PATH'])\n",
    "pik3ca_aff = pd.read_csv(project_paths['PIK3CA_DATA_PATH'])\n",
    "\n",
    "# fold split dictionaries\n",
    "with open(project_paths['KOR_FOLD_JSON'], 'r') as f:\n",
    "    kor_fs = json.load(f)\n",
    "with open(project_paths['PIK3CA_FOLD_JSON'],'r') as f:\n",
    "    pik3ca_fs = json.load(f)\n",
    "\n",
    "# validation and test set fold idx\n",
    "kvf, pvf = str(expset_obj.get_setting('kor-pred-best-cv')), str(expset_obj.get_setting('pik3ca-pred-best-cv'))\n",
    "ktf, ptf = str(global_settings.TEST_FOLD_IDX), str(global_settings.TEST_FOLD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building pre-training dataset\n",
    "kor_vinds, kor_tinds = np.array(kor_fs[kvf]), np.array(kor_fs[ktf])\n",
    "kor_excl = np.append(kor_vinds, kor_tinds)  # validation and test of KOR\n",
    "kor_excl_smis = kor_aff['smiles'].iloc[kor_excl].tolist()  # exclusion from KOR\n",
    "\n",
    "pik3ca_vinds, pik3ca_tinds = np.array(pik3ca_fs[pvf]), np.array(pik3ca_fs[ptf])\n",
    "pik3ca_excl = np.append(pik3ca_vinds, pik3ca_tinds)   # validation and test of PIK3CA\n",
    "pik3ca_excl_smis = pik3ca_aff['smiles'].iloc[pik3ca_excl].tolist()  # exclusion from PIK3CA\n",
    "\n",
    "excl_smis = kor_excl_smis.copy()\n",
    "excl_smis.extend(pik3ca_excl_smis.copy())\n",
    "set_excl_smis = set(excl_smis)\n",
    "\n",
    "with open(project_paths['CHEMBL_DATA_PATH'], 'r') as f:\n",
    "    new_chembl = [line.strip() for line in f.readlines()]\n",
    "# exclude the molecules from bioassay validation and test\n",
    "prior_smis = list(set(new_chembl).difference(set_excl_smis))\n",
    "with open(project_paths['PRETRAINING_DATA_PATH'], 'w') as f:\n",
    "    f.writelines([line+'\\n' for line in prior_smis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d159d7",
   "metadata": {},
   "source": [
    "Perform pre-training to build prior generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior generator training config\n",
    "config = global_settings.Object()\n",
    "config.tokens_path = project_paths['SMILES_TOKENS_PATH']\n",
    "config.pretrain_setting_path = project_paths['PRETRAIN_SETTING_JSON']\n",
    "config.dataset_path = project_paths['PRETRAINING_DATA_PATH']\n",
    "config.max_epoch = 20\n",
    "\n",
    "config.save_ckpt_fmt = project_paths['PROJECT_DIR'] + 'model-prior/prior_e%d.ckpt'\n",
    "config.sample_fmt = project_paths['PROJECT_DIR'] + 'model-prior/prior_e%d.txt'\n",
    "config.sample_size = 20000\n",
    "\n",
    "config.device_name = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform pre-training\n",
    "generator.pretrain(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdd956",
   "metadata": {},
   "source": [
    "Load the prior generator and sample some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_obj = smiles_vocab.Vocabulary(init_from_file=config.tokens_path)\n",
    "smtk = smiles_vocab.SmilesTokenizer(vocab_obj)\n",
    "\n",
    "with open(config.pretrain_setting_path, 'r') as f:\n",
    "    model_setting = json.load(f)\n",
    "    \n",
    "# load prior model (epoch=10)\n",
    "pret_ckpt = torch.load(config.save_ckpt_fmt%10, map_location='cpu')\n",
    "lstm_prior = smiles_lstm.SmilesLSTMGenerator(vocab_obj, model_setting['emb_size'], model_setting['hidden_units'], device_name='cpu')\n",
    "lstm_prior.lstm.load_state_dict(pret_ckpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e4891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sampling\n",
    "ssplr = analysis.SafeSampler(lstm_prior, batch_size=16)\n",
    "generated_smiles = ssplr.sample_clean(50, maxlen=150)\n",
    "display(generated_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4cc00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logics_shep8",
   "language": "python",
   "name": "logics_shep8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
